{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e9eb52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m     16\u001b[39m load_dotenv()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m api_key = \u001b[43mos\u001b[49m.getenv(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m model = ChatGoogleGenerativeAI(model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m,google_api_key=api_key,)\n\u001b[32m     19\u001b[39m model.invoke(\u001b[33m\"\u001b[39m\u001b[33mHello, how are you?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.messages import AnyMessage, AIMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key,)\n",
    "model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cee74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e2c69b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API Key not found. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API Key not found. Please pass a valid API key.'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3040\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3039\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3040\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\google\\genai\\models.py:5203\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5202\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5203\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5207\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\google\\genai\\models.py:3985\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3983\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3985\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   3990\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3991\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1385\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m response_body = (\n\u001b[32m   1390\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1194\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m     method=http_request.method,\n\u001b[32m   1196\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API Key not found. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API Key not found. Please pass a valid API key.'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m model = ChatGoogleGenerativeAI(model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m,google_api_key=api_key,)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, how are you?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2529\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2526\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2527\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2529\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3044\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3040\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   3041\u001b[39m         **request,\n\u001b[32m   3042\u001b[39m     )\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\campux\\LangGraph\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemini-2.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API Key not found. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API Key not found. Please pass a valid API key.'}]}}"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.messages import AnyMessage, AIMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "import os \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",google_api_key=api_key,)\n",
    "model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25594d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec69710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! I am functioning well and ready to assist you.\\n\\nHow may I help you today?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b366d-d1d5-7d23-90fd-d6a9de6b29c9-0', usage_metadata={'input_tokens': 7, 'output_tokens': 48, 'total_tokens': 55, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 28}})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab4fc30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "\n",
    "    messages: Annotated[list[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e4e6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ChatState):\n",
    "\n",
    "    decision = interrupt({\n",
    "        \"type\": \"approval\",\n",
    "        \"reason\": \"Model is about to answer a user question.\",\n",
    "        \"question\": state[\"messages\"][-1].content,\n",
    "        \"instruction\": \"Approve this question? yes/no\"\n",
    "    })\n",
    "    \n",
    "    if decision[\"approved\"] == 'no':\n",
    "        return {\"messages\": [AIMessage(content=\"Not approved.\")]}\n",
    "\n",
    "    else:\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09cfaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the graph: START -> chat -> END\n",
    "builder = StateGraph(ChatState)\n",
    "\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "\n",
    "builder.add_edge(START, \"chat\")\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "# Checkpointer is required for interrupts\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile the app\n",
    "app = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06382dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001F561241EE0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "684ffede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new thread id for this conversation\n",
    "config = {\"configurable\": {\"thread_id\": '1234'}}\n",
    "\n",
    "# ---- STEP 1: user asks a question ----\n",
    "initial_input = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Explain gradient descent in very simple terms.\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Invoke the graph for the first time\n",
    "result = app.invoke(initial_input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f500128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Explain gradient descent in very simple terms.', additional_kwargs={}, response_metadata={}, id='e6230987-ee96-4263-992c-17ab2641f136')],\n",
       " '__interrupt__': [Interrupt(value={'type': 'approval', 'reason': 'Model is about to answer a user question.', 'question': 'Explain gradient descent in very simple terms.', 'instruction': 'Approve this question? yes/no'}, id='fdf30d9c45061484c82826d5414a35a8')]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "655868e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'approval',\n",
       " 'reason': 'Model is about to answer a user question.',\n",
       " 'question': 'Explain gradient descent in very simple terms.',\n",
       " 'instruction': 'Approve this question? yes/no'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = result['__interrupt__'][0].value\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "240e14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = input(f\"\\nBackend message - {message} \\n Approve this question? (y/n): \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df3138c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume the graph with the approval decision\n",
    "final_result = app.invoke(\n",
    "    Command(resume={\"approved\": user_input}),\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7eb0cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're a **blindfolded person** standing on a **hilly landscape**.\n",
      "\n",
      "Your goal is to find the **absolute lowest point in the valley** (which represents the \"best solution\" or \"minimum error\" for a problem). But you can't see!\n",
      "\n",
      "Here's how you'd do it, which is exactly how Gradient Descent works:\n",
      "\n",
      "1.  **Feel the Slope:** You stand still for a moment and carefully feel the ground directly under your feet. \"Is it sloping up this way? Down that way? Flat here?\" You figure out the direction where the ground slopes **downwards the steepest**. This \"feeling the slope\" is the **\"gradient\"** part.\n",
      "\n",
      "2.  **Take a Small Step:** Once you know the steepest downhill direction, you take a **small step** in that direction. This is the **\"descent\"** part. You don't take a huge leap, because you might overshoot the bottom or step off a cliff you can't see!\n",
      "\n",
      "3.  **Repeat:** You repeat the process:\n",
      "    *   Stop, feel the new slope.\n",
      "    *   Take another small step in the new steepest downhill direction.\n",
      "\n",
      "You keep doing this, step by step, always moving in the direction that takes you further down.\n",
      "\n",
      "**When do you stop?** When you feel that the ground is completely flat all around you, no matter which way you try to feel. That means you've reached the bottom of the valley, and you can't go any lower!\n",
      "\n",
      "**In a nutshell:**\n",
      "\n",
      "Gradient Descent is an algorithm that iteratively finds the best solution by always moving in the direction that most quickly reduces the \"error\" or \"cost,\" taking small steps until it can't improve anymore.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(final_result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc4576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
